{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Dependecies\n!pip install transformers\n!pip install datasets\n!pip install trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Загрузка статей с Википедии по теме \"формальные языки\"\ndataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\nsubset = dataset.filter(lambda x: \"formal language\" in x['text'] or \"regular expressions\" in x['text'])\n\nprint(subset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nfrom datasets import load_dataset \nfrom trl import SFTTrainer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\ntrainer = SFTTrainer(\n    model, \n    train_dataset=subset,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n)\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Загрузка токенизатора для DistilGPT-2\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n\ndef ask_question(model, question, max_length=50):\n    # Токенизация вопроса\n    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n    \n    # Генерация ответа моделью\n    outputs = model.generate(\n        inputs['input_ids'], \n        max_length=max_length, \n        num_beams=5, \n        early_stopping=True\n    )\n    \n    # Декодирование ответа\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return answer\n\nquestion = \"what is regular expression?\"\n\nanswer = ask_question(model, question)\n\nprint(\"Question:\", question)\nprint(\"Answer:\", answer)\n","metadata":{},"execution_count":null,"outputs":[]}]}