{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T20:38:24.300140Z","iopub.status.busy":"2024-10-01T20:38:24.299742Z","iopub.status.idle":"2024-10-01T20:38:37.262076Z","shell.execute_reply":"2024-10-01T20:38:37.260804Z","shell.execute_reply.started":"2024-10-01T20:38:24.300097Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -U bitsandbytes"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T20:38:02.657941Z","iopub.status.busy":"2024-10-01T20:38:02.657543Z","iopub.status.idle":"2024-10-01T20:38:24.297624Z","shell.execute_reply":"2024-10-01T20:38:24.296369Z","shell.execute_reply.started":"2024-10-01T20:38:02.657901Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.2)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\n","Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.0)\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Collecting peft\n","  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2, einops, bitsandbytes, peft\n","Successfully installed PyPDF2-3.0.1 bitsandbytes-0.44.1 einops-0.8.0 peft-0.13.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install transformers accelerate einops bitsandbytes torch datasets PyPDF2 peft\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from huggingface_hub import login\n","\n","login(token='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')\n","\n","# Указываем идентификатор модели\n","model_id = \"meta-llama/Meta-Llama-3-8B\"\n","\n","# Проверяем доступность GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Настраиваем квантование\n","# Настраиваем квантование в 8 бит\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit=True,  # Включаем 8-битное квантование\n","    bnb_8bit_use_double_quant=True,  # Используем двойное квантование\n","    bnb_8bit_quant_type='nf4',  # Тип квантования (nf4 или fp4)\n","    bnb_8bit_compute_dtype=torch.float16  # Используем float16 для совместимости\n",")\n","# Загружаем токенизатор и модель с использованием квантования\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",  # Автоматическое распределение по доступным устройствам\n","    torch_dtype=torch.bfloat16  # Используем bfloat16 для ускорения\n",")\n","\n","# Переключаем модель в режим оценки (evaluation)\n","model.eval()\n","\n","# Пример запроса к модели\n","input_text = \"Hello, how are you today?\"\n","inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","\n","# Генерируем текст\n","with torch.no_grad():\n","    outputs = model.generate(**inputs, max_length=50)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["def ask_model(model, tokenizer, question, device='cuda', max_length=200):\n","    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_length=max_length,\n","            temperature=0.5,   # Контролирует креативность: чем ниже, тем детерминированнее\n","            top_k=30,          # Сокращает набор токенов для выбора\n","            top_p=0.8,         # Ограничивает сумму вероятностей для выбора токенов\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","    \n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","# Пример вопроса по ТФЯ\n","question = \"Ограничения LL-грамматик\"\n","response = ask_model(model, tokenizer, question)\n","print(f\"Question: {question}\\nAnswer: {response}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["**Подтягивание модели**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T20:39:34.010725Z","iopub.status.busy":"2024-10-01T20:39:34.010295Z","iopub.status.idle":"2024-10-01T20:39:34.206486Z","shell.execute_reply":"2024-10-01T20:39:34.205396Z","shell.execute_reply.started":"2024-10-01T20:39:34.010686Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Unused kwargs: ['bnb_8bit_use_double_quant', 'bnb_8bit_quant_type', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"]},{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["import PyPDF2\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n","from datasets import Dataset\n","from huggingface_hub import login\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","# Вход в Hugging Face\n","login(token='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')\n","\n","# Указываем идентификатор модели\n","model_id = \"meta-llama/Meta-Llama-3-8B\"\n","\n","# Проверяем доступность GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Настраиваем квантование в 8 бит\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit=True,  # Включаем 8-битное квантование\n","    bnb_8bit_use_double_quant=True,  # Используем двойное квантование\n","    bnb_8bit_quant_type='nf4',  # Тип квантования (nf4 или fp4)\n","    bnb_8bit_compute_dtype=torch.float16  # Используем float16 для совместимости\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",  # Автоматическое распределение по доступным устройствам\n","    torch_dtype=torch.float16  # Используем float16 для ускорения\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**Файнтюн**"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T20:50:31.626927Z","iopub.status.busy":"2024-10-01T20:50:31.626447Z","iopub.status.idle":"2024-10-01T20:51:49.492599Z","shell.execute_reply":"2024-10-01T20:51:49.491263Z","shell.execute_reply.started":"2024-10-01T20:50:31.626883Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracting text from /kaggle/input/okhotin/okhotin_fg_2019_l12.pdf...\n","Starting fine-tuning...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af2ed0ef2c5347249b93bfbca8d586d3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 00:00, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Model fine-tuned and saved!\n","Fine-tuning complete!\n","Generating answer...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Question: Ответь на русском: Какие ограничения существуют у LL-грамматик?\n","Answer: Ответь на русском: Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик? Какие ограничения существуют у LL-грамматик?\n"]}],"source":["import PyPDF2\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n","from datasets import Dataset\n","from huggingface_hub import login\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","# Функция для извлечения текста из PDF\n","def extract_text_from_pdf(pdf_path):\n","    with open(pdf_path, 'rb') as file:\n","        reader = PyPDF2.PdfReader(file)\n","        text = \"\"\n","        for page_num in range(len(reader.pages)):\n","            page = reader.pages[page_num]\n","            text += page.extract_text()\n","    return text\n","\n","\n","def ask_model(model, tokenizer, question, device='cuda', max_length=200):\n","    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_length=max_length,\n","            temperature=0.5,   # Контролирует креативность: чем ниже, тем детерминированнее\n","            top_k=30,          # Сокращает набор токенов для выбора\n","            top_p=0.8,         # Ограничивает сумму вероятностей для выбора токенов\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","    \n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","pdf_path = \"/input/okhotin_fg_2019_l12.pdf\"\n","print(f\"Extracting text from {pdf_path}...\")\n","text = extract_text_from_pdf(pdf_path)\n","\n","train_texts = [text]\n","\n","print(\"Starting fine-tuning...\")\n","\n","# Загружаем токенизатор и модель\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# Добавляем паддинг токен, если его нет\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","\n","# Настройка PEFT для LORA-файнтюнинга\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# Подготовка модели к обучению с использованием квантования и LORA\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, lora_config)\n","\n","# Токенизация текстов\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=512)\n","\n","# Создаем датасет из текста\n","dataset = Dataset.from_dict({\"text\": train_texts})\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","\n","# Параметры тренировки\n","training_args = TrainingArguments(\n","    output_dir=\"./fine_tuned_model\",\n","    evaluation_strategy=\"no\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=2,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    push_to_hub=False,\n","    logging_dir='./logs',\n",")\n","\n","# Добавляем специальный токен паддинга в модель (если был добавлен в токенизатор)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Data collator для автоматической генерации масок и шифтов (используется для MLM или casual LM)\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,  # Для языкового моделирования (casual LM), используем mlm=False\n",")\n","\n","# Создаем тренера\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n","    data_collator=data_collator,  # Добавляем data_collator для корректной генерации данных\n",")\n","\n","# Запуск fine-tuning\n","trainer.train()\n","\n","# Сохранение модели\n","trainer.save_model(\"./fine_tuned_model\")\n","tokenizer.save_pretrained(\"./fine_tuned_model\")\n","\n","print(\"Model fine-tuned and saved!\")\n","print(\"Fine-tuning complete!\")\n","# Пример вопроса по ТФЯ\n","print(\"Generating answer...\")\n","question = \"Ответь на русском: Какие ограничения существуют у LL-грамматик?\"\n","response = ask_model(model, tokenizer, question)\n","print(f\"Question: {question}\\nAnswer: {response}\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**Вопрос**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from time import time\n","def ask_model(model, tokenizer, question, device='cuda', max_length=200):\n","    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_length=max_length,\n","            temperature=0.5,   # Контролирует креативность: чем ниже, тем детерминированнее\n","            top_k=30,          # Сокращает набор токенов для выбора\n","            top_p=0.8,         # Ограничивает сумму вероятностей для выбора токенов\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","    \n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","start = time()\n","# Пример вопроса по ТФЯ\n","question = \"Ответь на русском: Какие ограничения существуют у LL-грамматик?\"\n","response = ask_model(model, tokenizer, question)\n","end = time()\n","print(f\"Question: {question}\\nAnswer: {response}\")\n","\n","print(f\"answered in: {round(end-start, 3)} sec.\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5801189,"sourceId":9526649,"sourceType":"datasetVersion"},{"datasetId":5801280,"sourceId":9526765,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
