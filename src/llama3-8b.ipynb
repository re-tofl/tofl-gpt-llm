{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T20:38:24.300140Z","iopub.status.busy":"2024-10-01T20:38:24.299742Z","iopub.status.idle":"2024-10-01T20:38:37.262076Z","shell.execute_reply":"2024-10-01T20:38:37.260804Z","shell.execute_reply.started":"2024-10-01T20:38:24.300097Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -U bitsandbytes"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T20:38:02.657941Z","iopub.status.busy":"2024-10-01T20:38:02.657543Z","iopub.status.idle":"2024-10-01T20:38:24.297624Z","shell.execute_reply":"2024-10-01T20:38:24.296369Z","shell.execute_reply.started":"2024-10-01T20:38:02.657901Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.2)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\n","Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.0)\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Collecting peft\n","  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2, einops, bitsandbytes, peft\n","Successfully installed PyPDF2-3.0.1 bitsandbytes-0.44.1 einops-0.8.0 peft-0.13.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install transformers accelerate einops bitsandbytes torch datasets PyPDF2 peft\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from huggingface_hub import login\n","\n","login(token='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')\n","\n","# –£–∫–∞–∑—ã–≤–∞–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏\n","model_id = \"meta-llama/Meta-Llama-3-8B\"\n","\n","# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ\n","# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –≤ 8 –±–∏—Ç\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit=True,  # –í–∫–ª—é—á–∞–µ–º 8-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ\n","    bnb_8bit_use_double_quant=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ\n","    bnb_8bit_quant_type='nf4',  # –¢–∏–ø –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (nf4 –∏–ª–∏ fp4)\n","    bnb_8bit_compute_dtype=torch.float16  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\n",")\n","# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º\n","    torch_dtype=torch.bfloat16  # –ò—Å–ø–æ–ª—å–∑—É–µ–º bfloat16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",")\n","\n","# –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ (evaluation)\n","model.eval()\n","\n","# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏\n","input_text = \"Hello, how are you today?\"\n","inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","\n","# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n","with torch.no_grad():\n","    outputs = model.generate(**inputs, max_length=50)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["def ask_model(model, tokenizer, question, device='cuda', max_length=200):\n","    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_length=max_length,\n","            temperature=0.5,   # –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å: —á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–µ–µ\n","            top_k=30,          # –°–æ–∫—Ä–∞—â–∞–µ—Ç –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∞\n","            top_p=0.8,         # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Å—É–º–º—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","    \n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","# –ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞ –ø–æ –¢–§–Ø\n","question = \"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫\"\n","response = ask_model(model, tokenizer, question)\n","print(f\"Question: {question}\\nAnswer: {response}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["**–ü–æ–¥—Ç—è–≥–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T20:39:34.010725Z","iopub.status.busy":"2024-10-01T20:39:34.010295Z","iopub.status.idle":"2024-10-01T20:39:34.206486Z","shell.execute_reply":"2024-10-01T20:39:34.205396Z","shell.execute_reply.started":"2024-10-01T20:39:34.010686Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Unused kwargs: ['bnb_8bit_use_double_quant', 'bnb_8bit_quant_type', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"]},{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["import PyPDF2\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n","from datasets import Dataset\n","from huggingface_hub import login\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","# –í—Ö–æ–¥ –≤ Hugging Face\n","login(token='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')\n","\n","# –£–∫–∞–∑—ã–≤–∞–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏\n","model_id = \"meta-llama/Meta-Llama-3-8B\"\n","\n","# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –≤ 8 –±–∏—Ç\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit=True,  # –í–∫–ª—é—á–∞–µ–º 8-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ\n","    bnb_8bit_use_double_quant=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ\n","    bnb_8bit_quant_type='nf4',  # –¢–∏–ø –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (nf4 –∏–ª–∏ fp4)\n","    bnb_8bit_compute_dtype=torch.float16  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º\n","    torch_dtype=torch.float16  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**–§–∞–π–Ω—Ç—é–Ω**"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T20:50:31.626927Z","iopub.status.busy":"2024-10-01T20:50:31.626447Z","iopub.status.idle":"2024-10-01T20:51:49.492599Z","shell.execute_reply":"2024-10-01T20:51:49.491263Z","shell.execute_reply.started":"2024-10-01T20:50:31.626883Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracting text from /kaggle/input/okhotin/okhotin_fg_2019_l12.pdf...\n","Starting fine-tuning...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af2ed0ef2c5347249b93bfbca8d586d3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 00:00, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Model fine-tuned and saved!\n","Fine-tuning complete!\n","Generating answer...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Question: –û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º: –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫?\n","Answer: –û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º: –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫? –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫?\n"]}],"source":["import PyPDF2\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n","from datasets import Dataset\n","from huggingface_hub import login\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF\n","def extract_text_from_pdf(pdf_path):\n","    with open(pdf_path, 'rb') as file:\n","        reader = PyPDF2.PdfReader(file)\n","        text = \"\"\n","        for page_num in range(len(reader.pages)):\n","            page = reader.pages[page_num]\n","            text += page.extract_text()\n","    return text\n","\n","\n","def ask_model(model, tokenizer, question, device='cuda', max_length=200):\n","    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_length=max_length,\n","            temperature=0.5,   # –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å: —á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–µ–µ\n","            top_k=30,          # –°–æ–∫—Ä–∞—â–∞–µ—Ç –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∞\n","            top_p=0.8,         # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Å—É–º–º—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","    \n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","pdf_path = \"/input/okhotin_fg_2019_l12.pdf\"\n","print(f\"Extracting text from {pdf_path}...\")\n","text = extract_text_from_pdf(pdf_path)\n","\n","train_texts = [text]\n","\n","print(\"Starting fine-tuning...\")\n","\n","# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–¥–¥–∏–Ω–≥ —Ç–æ–∫–µ–Ω, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","\n","# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ PEFT –¥–ª—è LORA-—Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –∏ LORA\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, lora_config)\n","\n","# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=512)\n","\n","# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞\n","dataset = Dataset.from_dict({\"text\": train_texts})\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","\n","# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n","training_args = TrainingArguments(\n","    output_dir=\"./fine_tuned_model\",\n","    evaluation_strategy=\"no\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=2,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    push_to_hub=False,\n","    logging_dir='./logs',\n",")\n","\n","# –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω –ø–∞–¥–¥–∏–Ω–≥–∞ –≤ –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Data collator –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∞—Å–æ–∫ –∏ —à–∏—Ñ—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è MLM –∏–ª–∏ casual LM)\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,  # –î–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (casual LM), –∏—Å–ø–æ–ª—å–∑—É–µ–º mlm=False\n",")\n","\n","# –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n","    data_collator=data_collator,  # –î–æ–±–∞–≤–ª—è–µ–º data_collator –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö\n",")\n","\n","# –ó–∞–ø—É—Å–∫ fine-tuning\n","trainer.train()\n","\n","# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n","trainer.save_model(\"./fine_tuned_model\")\n","tokenizer.save_pretrained(\"./fine_tuned_model\")\n","\n","print(\"Model fine-tuned and saved!\")\n","print(\"Fine-tuning complete!\")\n","# –ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞ –ø–æ –¢–§–Ø\n","print(\"Generating answer...\")\n","question = \"–û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º: –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫?\"\n","response = ask_model(model, tokenizer, question)\n","print(f\"Question: {question}\\nAnswer: {response}\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**–í–æ–ø—Ä–æ—Å**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from time import time\n","def ask_model(model, tokenizer, question, device='cuda', max_length=200):\n","    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_length=max_length,\n","            temperature=0.5,   # –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å: —á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–µ–µ\n","            top_k=30,          # –°–æ–∫—Ä–∞—â–∞–µ—Ç –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∞\n","            top_p=0.8,         # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Å—É–º–º—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","    \n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","start = time()\n","# –ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞ –ø–æ –¢–§–Ø\n","question = \"–û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º: –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LL-–≥—Ä–∞–º–º–∞—Ç–∏–∫?\"\n","response = ask_model(model, tokenizer, question)\n","end = time()\n","print(f\"Question: {question}\\nAnswer: {response}\")\n","\n","print(f\"answered in: {round(end-start, 3)} sec.\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5801189,"sourceId":9526649,"sourceType":"datasetVersion"},{"datasetId":5801280,"sourceId":9526765,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
